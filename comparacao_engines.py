#!/usr/bin/env python3
"""
üî¨ COMPARA√á√ÉO DE PERFORMANCE - AI ENGINES
Teste comparativo entre diferentes vers√µes do AI Engine
"""

import pandas as pd
import numpy as np
import json
from datetime import datetime
import logging
import time

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def compare_ai_engines():
    """Comparar performance dos diferentes engines"""
    
    print("üî¨ COMPARA√á√ÉO DE PERFORMANCE - AI ENGINES")
    print("=" * 70)
    
    try:
        from src.config import Config
        from src.market_data import MarketDataManager
        from ai_engine_ultra_enhanced import UltraEnhancedAIEngine
        from ai_engine_v3_otimizado import OptimizedAIEngineV3
        
        config = Config()
        market_data = MarketDataManager(config)
        
        # Engines para testar
        engines = {
            'Ultra Enhanced': UltraEnhancedAIEngine(config),
            'Optimized V3': OptimizedAIEngineV3(config)
        }
        
        # S√≠mbolos e timeframes para testar
        test_cases = [
            ('BTCUSDT', '1m'),
            ('BTCUSDT', '5m'),
            ('ETHUSDT', '1m'),
            ('ETHUSDT', '5m'),
            ('BNBUSDT', '1m'),
            ('BNBUSDT', '5m'),
        ]
        
        results = {}
        
        for symbol, timeframe in test_cases:
            print(f"\nüìä Testando {symbol} {timeframe}")
            print("-" * 50)
            
            # Obter dados
            df = market_data.get_historical_data(symbol, timeframe, 500)
            if df is None or len(df) < 200:
                print(f"‚ùå Dados insuficientes para {symbol} {timeframe}")
                continue
            
            test_key = f"{symbol}_{timeframe}"
            results[test_key] = {}
            
            for engine_name, engine in engines.items():
                print(f"\nüß† Testando {engine_name}...")
                
                start_time = time.time()
                
                try:
                    # Fazer predi√ß√£o
                    if engine_name == 'Ultra Enhanced':
                        result = engine.ultra_predict_signal(df, f"{symbol}_{timeframe}")
                    else:  # Optimized V3
                        result = engine.optimized_predict_signal(df, f"{symbol}_{timeframe}")
                    
                    end_time = time.time()
                    execution_time = end_time - start_time
                    
                    # Extrair m√©tricas
                    signal_type = result.get('signal_type', 'HOLD')
                    confidence = result.get('confidence', 0)
                    model_accuracy = result.get('model_accuracy', 0)
                    confluence = result.get('confluence', 0)
                    
                    results[test_key][engine_name] = {
                        'signal_type': signal_type,
                        'confidence': confidence,
                        'model_accuracy': model_accuracy,
                        'confluence': confluence,
                        'execution_time': execution_time,
                        'success': True
                    }
                    
                    print(f"   ‚úÖ Sinal: {signal_type}")
                    print(f"   üìà Confian√ßa: {confidence:.3f}")
                    print(f"   üéØ Acur√°cia: {model_accuracy:.3f}")
                    print(f"   ‚è±Ô∏è Tempo: {execution_time:.2f}s")
                    
                except Exception as e:
                    results[test_key][engine_name] = {
                        'success': False,
                        'error': str(e),
                        'execution_time': time.time() - start_time
                    }
                    print(f"   ‚ùå Erro: {e}")
        
        # An√°lise comparativa
        print(f"\nüìä AN√ÅLISE COMPARATIVA")
        print("=" * 70)
        
        engine_stats = {}
        for engine_name in engines.keys():
            engine_stats[engine_name] = {
                'total_tests': 0,
                'successful_tests': 0,
                'active_signals': 0,
                'avg_confidence': 0,
                'avg_accuracy': 0,
                'avg_execution_time': 0,
                'signals_by_type': {'BUY': 0, 'SELL': 0, 'HOLD': 0}
            }
        
        # Calcular estat√≠sticas
        for test_key, test_results in results.items():
            for engine_name, result in test_results.items():
                stats = engine_stats[engine_name]
                stats['total_tests'] += 1
                stats['avg_execution_time'] += result.get('execution_time', 0)
                
                if result.get('success', False):
                    stats['successful_tests'] += 1
                    confidence = result.get('confidence', 0)
                    accuracy = result.get('model_accuracy', 0)
                    signal_type = result.get('signal_type', 'HOLD')
                    
                    stats['avg_confidence'] += confidence
                    stats['avg_accuracy'] += accuracy
                    stats['signals_by_type'][signal_type] += 1
                    
                    if signal_type != 'HOLD':
                        stats['active_signals'] += 1
        
        # Normalizar m√©dias
        for engine_name, stats in engine_stats.items():
            if stats['successful_tests'] > 0:
                stats['avg_confidence'] /= stats['successful_tests']
                stats['avg_accuracy'] /= stats['successful_tests']
            if stats['total_tests'] > 0:
                stats['avg_execution_time'] /= stats['total_tests']
        
        # Exibir resultados
        for engine_name, stats in engine_stats.items():
            print(f"\nüß† {engine_name.upper()}")
            print(f"   ‚úÖ Testes bem-sucedidos: {stats['successful_tests']}/{stats['total_tests']}")
            print(f"   üéØ Sinais ativos: {stats['active_signals']}")
            print(f"   üìà Confian√ßa m√©dia: {stats['avg_confidence']:.3f}")
            print(f"   üéØ Acur√°cia m√©dia: {stats['avg_accuracy']:.3f}")
            print(f"   ‚è±Ô∏è Tempo m√©dio: {stats['avg_execution_time']:.2f}s")
            print(f"   üìä Distribui√ß√£o: BUY:{stats['signals_by_type']['BUY']} SELL:{stats['signals_by_type']['SELL']} HOLD:{stats['signals_by_type']['HOLD']}")
            
            # Calcular score geral
            success_rate = stats['successful_tests'] / max(stats['total_tests'], 1)
            signal_activity = stats['active_signals'] / max(stats['successful_tests'], 1)
            overall_score = (stats['avg_confidence'] * 0.4 + 
                           stats['avg_accuracy'] * 0.4 + 
                           success_rate * 0.1 + 
                           signal_activity * 0.1)
            
            print(f"   üèÜ Score geral: {overall_score:.3f}")
        
        # Recomenda√ß√£o
        best_engine = max(engine_stats.keys(), 
                         key=lambda x: engine_stats[x]['avg_accuracy'] * engine_stats[x]['avg_confidence'])
        
        print(f"\nüèÜ RECOMENDA√á√ÉO:")
        print(f"   Engine recomendado: {best_engine}")
        print(f"   Raz√£o: Melhor combina√ß√£o de acur√°cia e confian√ßa")
        
        # Salvar resultados detalhados
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"comparacao_engines_{timestamp}.json"
        with open(filename, 'w') as f:
            json.dump({
                'timestamp': timestamp,
                'results': results,
                'statistics': engine_stats,
                'recommendation': best_engine
            }, f, indent=2, default=str)
        
        print(f"\nüíæ Resultados salvos em: {filename}")
        
        return results, engine_stats
        
    except Exception as e:
        print(f"‚ùå Erro na compara√ß√£o: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def analyze_signal_quality():
    """An√°lise de qualidade dos sinais gerados"""
    
    print(f"\nüîç AN√ÅLISE DE QUALIDADE DOS SINAIS")
    print("=" * 50)
    
    try:
        from src.config import Config
        from src.market_data import MarketDataManager
        from ai_engine_v3_otimizado import OptimizedAIEngineV3
        
        config = Config()
        market_data = MarketDataManager(config)
        optimized_ai = OptimizedAIEngineV3(config)
        
        # Teste com diferentes thresholds
        thresholds = [0.30, 0.35, 0.40, 0.45, 0.50]
        symbol = 'BTCUSDT'
        timeframe = '5m'
        
        print(f"üìä Analisando {symbol} {timeframe} com diferentes thresholds")
        
        df = market_data.get_historical_data(symbol, timeframe, 500)
        if df is None or len(df) < 200:
            print("‚ùå Dados insuficientes")
            return
        
        threshold_results = []
        
        for threshold in thresholds:
            print(f"\nüìè Threshold: {threshold}")
            
            # Configurar threshold
            optimized_ai.min_confidence_threshold = threshold
            
            # Fazer predi√ß√£o
            result = optimized_ai.optimized_predict_signal(df, f"{symbol}_{timeframe}")
            
            signal_type = result.get('signal_type', 'HOLD')
            confidence = result.get('confidence', 0)
            model_accuracy = result.get('model_accuracy', 0)
            
            threshold_results.append({
                'threshold': threshold,
                'signal_type': signal_type,
                'confidence': confidence,
                'model_accuracy': model_accuracy,
                'is_active': signal_type != 'HOLD'
            })
            
            print(f"   üéØ Sinal: {signal_type}")
            print(f"   üìà Confian√ßa: {confidence:.3f}")
            print(f"   üéØ Acur√°cia: {model_accuracy:.3f}")
        
        # Encontrar threshold √≥timo
        active_signals = [r for r in threshold_results if r['is_active']]
        
        if active_signals:
            # Ordenar por confian√ßa * acur√°cia
            active_signals.sort(key=lambda x: x['confidence'] * x['model_accuracy'], reverse=True)
            best_signal = active_signals[0]
            
            print(f"\nüèÜ MELHOR CONFIGURA√á√ÉO:")
            print(f"   Threshold: {best_signal['threshold']}")
            print(f"   Sinal: {best_signal['signal_type']}")
            print(f"   Confian√ßa: {best_signal['confidence']:.3f}")
            print(f"   Acur√°cia: {best_signal['model_accuracy']:.3f}")
            print(f"   Score: {best_signal['confidence'] * best_signal['model_accuracy']:.3f}")
        else:
            print(f"\n‚ö†Ô∏è Nenhum sinal ativo encontrado")
            print(f"   Recomenda√ß√£o: Usar threshold mais baixo (<{min(thresholds)})")
        
        return threshold_results
        
    except Exception as e:
        print(f"‚ùå Erro na an√°lise: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    # Executar compara√ß√£o
    results, stats = compare_ai_engines()
    
    # Executar an√°lise de qualidade
    quality_results = analyze_signal_quality()
    
    print(f"\n‚úÖ TESTES CONCLU√çDOS!")
    print("=" * 50)
    print("üìä Engines comparados")
    print("üîç Qualidade dos sinais analisada")
    print("üíæ Resultados salvos em arquivos JSON")
